ClearVue Streaming Pipeline - Quick Start Guide
ğŸ¯ Choose Your MongoDB Setup
You have TWO options for where to store your data:

Option 1: MongoDB Atlas (Cloud) â˜ï¸ RECOMMENDED
âœ… Already configured
âœ… No local setup needed
âœ… Works anywhere with internet
âœ… Free tier available
Option 2: Local Docker MongoDB ğŸ³
âœ… Fully offline
âœ… Complete control
âœ… No cloud dependencies
âš ï¸ Requires Docker setup
ğŸš€ Complete Setup (5 Minutes)
Step 1: Automated Setup
bash
# Make script executable
chmod +x setup_complete.sh

# Run setup wizard
./setup_complete.sh

# It will ask:
# "Where do you want to store your MongoDB data?"
# 1) MongoDB Atlas (Cloud) - Recommended
# 2) Local Docker MongoDB
Choose option 1 for Atlas (easiest) or option 2 for local Docker

The script will:

âœ… Install Python dependencies
âœ… Create Docker volumes & network
âœ… Start Kafka & Zookeeper
âœ… Start MongoDB (if local mode)
âœ… Create .env configuration file
Step 2: Load Your Data
bash
# Place your 17 CSV files in ./cleaned_data/
mkdir -p cleaned_data
# Copy: Trans_Types.csv, Suppliers.csv, etc.

# Transform and load (ONE-TIME operation)
python transform_and_load.py ./cleaned_data

# Expected output:
# âœ… Files loaded: 17/17
# âœ… Collections created: 6/6
# âœ… Documents inserted: X,XXX
Step 3: Start Streaming
bash
# Terminal 1: Start the pipeline
python clearvue_streaming_pipeline_final.py
# Choose: 1 (ALL collections)

# You should see:
# ğŸŸ¢ PIPELINE IS RUNNING!
# ğŸ‘ï¸ Watching: sales, payments, purchases...
Step 4: Test It
bash
# Terminal 2: Generate test transactions
python transaction_simulator_enhanced.py
# Choose: 1 (Mixed burst)

# Watch Terminal 1 for:
# âš¡ SALES: insert detected
# âš¡ PAYMENT: insert detected
Step 5: Verify
Open in browser:

Kafka UI: http://localhost:8080 (see messages)
Mongo Express: http://localhost:8081 (see data)
ğŸ“Š What Gets Created
6 MongoDB Collections:
customers - Customer master with embedded age_analysis
suppliers - Supplier information
products - Products with embedded brand/category/style
sales - Sales transactions with embedded lines
payments - Payment transactions with embedded lines
purchases - Purchase orders with embedded lines
6 Kafka Topics:
clearvue.sales.realtime - Real-time sales
clearvue.payments.realtime - Real-time payments
clearvue.purchases.realtime - Real-time purchases
clearvue.customers.changes - Customer updates
clearvue.products.changes - Product updates
clearvue.suppliers.changes - Supplier updates
ğŸ® Usage Examples
Generate Different Transaction Types
bash
python transaction_simulator_enhanced.py

# Options:
1. Mixed burst (20 transactions)      # â† Best for testing
2. Continuous (10 tx/min)             # â† For demo
3. Heavy load (30 tx/min)             # â† Stress test
4. Sales only
5. Payments only
6. Master data updates only
Test Kafka Messages
bash
python kafka_consumer_test.py

# Choose: 7 (ALL topics)
# Then run simulator in another terminal
# Watch formatted messages appear
Switch MongoDB Mode
bash
# To use Atlas:
export MONGODB_MODE=atlas
python transform_and_load.py ./cleaned_data

# To use Local Docker:
export MONGODB_MODE=local
python transform_and_load.py ./cleaned_data
ğŸ” Verify Everything Works
Check 1: Docker Services
bash
docker-compose ps

# Should show (healthy):
# clearvue_kafka
# clearvue_zookeeper
# clearvue_mongodb (if local mode)
Check 2: MongoDB Data
bash
# If using Atlas:
mongosh "mongodb+srv://Tyra:1234@novacluster.1re1a4e.mongodb.net/Nova_Analytics"

# If using Local:
mongosh "mongodb://admin:clearvue123@localhost:27017/clearvue_bi"

# Then:
show collections
db.sales.countDocuments()
db.payments.countDocuments()
Check 3: Kafka Topics
Visit: http://localhost:8080

Click "Topics"
You should see 6 clearvue topics
Click one to see messages (after running simulator)
Check 4: Pipeline Logs
bash
tail -f streaming_pipeline.log

# Should show:
# INFO - Started change stream: sales
# INFO - Started change stream: payments
# ...
ğŸ› Quick Troubleshooting
Problem: "Connection failed"
bash
# Check Docker is running:
docker ps

# Restart services:
docker-compose restart

# Check logs:
docker-compose logs kafka
docker-compose logs mongodb
Problem: "No collections found"
bash
# Did you run the transformation?
python transform_and_load.py ./cleaned_data

# Check if files exist:
ls -la cleaned_data/
Problem: "Pipeline not detecting changes"
bash
# Make sure pipeline is running:
ps aux | grep streaming_pipeline

# Generate a test transaction:
python transaction_simulator_enhanced.py
# Choose: 4 (Sales only)

# Check Kafka UI immediately:
# http://localhost:8080
Problem: "Kafka topics empty"
bash
# Make sure pipeline is running FIRST
python clearvue_streaming_pipeline_final.py

# THEN generate transactions
python transaction_simulator_enhanced.py
ğŸ¬ For Your Demo Video
bash
# Terminal 1: Start pipeline
python clearvue_streaming_pipeline_final.py

# Terminal 2: Run simulator
python transaction_simulator_enhanced.py
# Choose: 2 (Continuous)

# Show in video:
1. Terminal 1 - Pipeline detecting changes
2. Terminal 2 - Simulator generating transactions
3. Browser - Kafka UI showing messages (http://localhost:8080)
4. Browser - MongoDB data (http://localhost:8081)
5. Power BI - Dashboard updating (if connected)
ğŸ“ Project File Structure
clearvue-streaming/
â”œâ”€â”€ config.py                       # Configuration manager
â”œâ”€â”€ setup_complete.sh               # Setup wizard
â”œâ”€â”€ docker-compose.yml              # Infrastructure
â”œâ”€â”€ transform_and_load.py           # Data transformation
â”œâ”€â”€ clearvue_streaming_pipeline_final.py  # Main pipeline
â”œâ”€â”€ transaction_simulator_enhanced.py     # Test data generator
â”œâ”€â”€ kafka_consumer_test.py          # Kafka message tester
â”œâ”€â”€ .env                            # Your config (auto-generated)
â”œâ”€â”€ streaming_pipeline.log          # Pipeline logs
â”œâ”€â”€ cleaned_data/                   # Your 17 CSV files
â”‚   â”œâ”€â”€ Trans_Types.csv
â”‚   â”œâ”€â”€ Suppliers.csv
â”‚   â””â”€â”€ ... (15 more files)
â”œâ”€â”€ QUICK_START.md                  # This file
â””â”€â”€ README.md                       # Full documentation
ğŸ’¡ Pro Tips
Always start pipeline BEFORE simulator
Pipeline must be watching for changes
Check Kafka UI frequently
http://localhost:8080
Verify messages are flowing
Use Mixed burst for testing
Tests all 6 collections
Shows variety of transactions
Export Kafka schema for Power BI team
bash
   python kafka_consumer_test.py
   # Choose: Export schema
   # Share kafka_message_schema.json
Monitor pipeline stats
Stats print every 30 seconds
Shows what's being processed
ğŸ†˜ Need Help?
Check Logs
bash
# Pipeline logs
tail -f streaming_pipeline.log

# Docker logs
docker-compose logs -f kafka
docker-compose logs -f mongodb

# All logs
docker-compose logs --tail=100
Health Checks
bash
# Kafka
docker exec clearvue_kafka kafka-topics --bootstrap-server localhost:9092 --list

# MongoDB (local)
docker exec clearvue_mongodb mongosh --eval "db.adminCommand('ping')"

# Check everything
./setup_complete.sh  # Re-run setup
âœ… Success Checklist
Before your demo/presentation:

 Docker services running (docker-compose ps)
 17 CSV files in ./cleaned_data/
 Data transformed and loaded (6 collections exist)
 Pipeline running and watching collections
 Simulator generates transactions successfully
 Kafka UI shows messages (http://localhost:8080)
 Mongo Express shows data (http://localhost:8081)
 kafka_consumer_test.py displays messages
 Power BI connected (optional)
ğŸ“ Understanding the Flow
Your 17 CSV Files
        â†“
[transform_and_load.py] ONE-TIME
        â†“
6 MongoDB Collections
        â†“
[clearvue_streaming_pipeline_final.py] ALWAYS RUNNING
   (Watches for changes via Change Streams)
        â†“
Enriches with business context
        â†“
Apache Kafka Topics
        â†“
Power BI Dashboard (Real-time updates)
Key Point: The pipeline doesn't move data, it detects and broadcasts changes in real-time!

ğŸš€ You're Ready!
If you completed all steps above, you have:

âœ… Complete streaming infrastructure
âœ… 6 NoSQL collections with real data
âœ… Real-time change detection
âœ… Kafka streaming working
âœ… Test data generator
âœ… Message verification tool
Next: Connect Power BI to Kafka topics and build your dashboard!

